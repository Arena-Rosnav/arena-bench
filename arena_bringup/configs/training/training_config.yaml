# CONTAINS THE AGENT CONFIGURATION FOR TRAINING
agent_cfg:
  name: null # agent name (should be unique)

  # action space
  action_space:
    is_discrete: false
    custom_discretization: null

  framework:
    name: stable_baselines3  # framework name
    model:
      architecture_name: AGENT_1
      resume: null
        # agent_name: null  # name of the agent to resume training from
        # checkpoint: null  # checkpoint name 
        # path: null  # (optional) path to the model file - if null loading from default agent directory
    
    algorithm: # training algorithm configuration
      batch_size: 128  # Number of samples per gradient update
      clip_range: 0.2  # Clipping parameter for PPO
      clip_range_vf: null  # Clipping parameter for value function (if None, no clipping)
      device: auto  # Device to use for training (cpu, cuda, auto)
      ent_coef: 0.0  # Entropy coefficient for the loss calculation
      gae_lambda: 0.95  # Lambda for generalized advantage estimation
      gamma: 0.99  # Discount factor
      learning_rate:  # Learning rate schedule
        type: linear  # Type of learning rate schedule (linear, constant, etc.)
        kwargs:
          initial_value: 0.001  # Initial learning rate
          final_value: 0.0001  # Final learning rate
      max_grad_norm: 0.5  # Maximum norm for gradient clipping
      n_epochs: 5  # Number of epochs to update the policy
      n_steps: null  # Number of steps to run for each environment per update
      normalize_advantage: true  # Whether to normalize advantages
      sde_sample_freq: -1  # Frequency of sampling for state-dependent exploration (if -1, no sampling)
      seed: null  # Seed for the random number generator
      stats_window_size: 100  # Window size for the rolling statistics
      target_kl: null  # Target KL divergence threshold
      tensorboard_log: null  # Directory for tensorboard logs (if None, no logging)
      total_batch_size: 128  # Total batch size for training
      use_sde: false  # Whether to use state-dependent exploration
      verbose: 2  # Verbosity level (0: no output, 1: info, 2: debug)
      vf_coef: 0.22  # Coefficient for the value function loss

  # reward function
  reward:
    file_name: barn
    reward_unit_kwargs: null
    verbose: true # print reward values

# CONTAINS THE FRAMEWORK CONFIGURATION FOR TRAINING 
framework_cfg:
  # currently only Stable-Baselines3 implemented as framework
  # and PPO as algorithm

  # general training configuration
  general:
    # in debug_mode no agent directories will be created and no models will be saved
    # further no wandb logging and fake (simulated) multiprocessing for traceback
    debug_mode: true
    goal_radius: 0.5
    max_num_moves_per_eps: 175
    n_envs: 1 # number of parallel environments
    n_timesteps: 4000000
    no_gpu: false
    safety_distance: 1.0
    show_progress_bar: false

  # task modules
  task:
    tm_modules: staged
    tm_obstacles: random
    tm_robots: random

  monitoring:
    wandb: true  # weights and biases logging
    episode_logging:  # display episode statistics (avg. success rate, reward, eps length..)
      last_n_episodes: 30
      record_actions: true
    eval_metrics: false # save evaluation stats during training in log file
    training_metrics: true  # display training metrics

  callbacks:
    periodic_evaluation:
      eval_freq: 30000  # evaluation frequency, evaluation after every n_envs * eval_freq timesteps
      max_num_moves_per_eps: 250  # max number of steps per episode
      n_eval_episodes: 40 # number of evaluation episodes
    stop_training_on_threshold:
      threshold: 0.9
      threshold_type: succ  # can be either "succ" (success rate) or "rew" (reward)
      verbose: 1
    training_curriculum:
      current_stage: 0
      curriculum_file: default.yaml
      lower_threshold: 0.3
      threshold_type: succ  # can be either "succ" (success rate) or "rew" (reward)
      upper_threshold: 0.9

  normalization: null
  profiling: null
